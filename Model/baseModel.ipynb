{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6be35e-aaf8-4750-93e5-f4076ba7a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from google.cloud import storage\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b31101e-f99e-4805-8a04-5d17fa0b9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob(bucket_name, filename):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39da826-d8f9-41e4-82e3-6d102447eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://data_tql/Model/2023-05-09/t5-small/run_2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_number = 2\n",
    "model_run_date = datetime.today().strftime('%Y-%m-%d')\n",
    "model_name = 't5-small'\n",
    "model_folder = f'gs://data_tql/Model/{model_run_date}/{model_name}/run_'\n",
    "\n",
    "if blob('data_tql', model_folder).exists():\n",
    "    model_folder = model_folder + f'{run_number + 1}'\n",
    "else:\n",
    "    model_folder = model_folder + f'{run_number}'\n",
    "\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef33b6c9-5577-4e55-ab27-41c85185d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e0cead-4501-4614-8b39-3ec47cdfd09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>TQL</th>\n",
       "      <th>SQL</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fileName</th>\n",
       "      <th>filePath</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>How many heads of the departments are older th...</td>\n",
       "      <td>SELECT count(*) FROM head WHERE age  &gt;  56</td>\n",
       "      <td>train</td>\n",
       "      <td>department_management.sqlite</td>\n",
       "      <td>sqliteDB/department_management.sqlite</td>\n",
       "      <td>{'count(*)': {0: 5}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                                TQL  \\\n",
       "0  department_management  How many heads of the departments are older th...   \n",
       "\n",
       "                                          SQL dataset  \\\n",
       "0  SELECT count(*) FROM head WHERE age  >  56   train   \n",
       "\n",
       "                       fileName                               filePath  \\\n",
       "0  department_management.sqlite  sqliteDB/department_management.sqlite   \n",
       "\n",
       "                 result  \n",
       "0  {'count(*)': {0: 5}}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_id</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_original</th>\n",
       "      <th>primary_key</th>\n",
       "      <th>column_list</th>\n",
       "      <th>column_list_original</th>\n",
       "      <th>column_datatypes</th>\n",
       "      <th>foreign_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>Perpetrator_ID</td>\n",
       "      <td>['perpetrator id', 'people id', 'date', 'year'...</td>\n",
       "      <td>['Perpetrator_ID', 'People_ID', 'Date', 'Year'...</td>\n",
       "      <td>['number', 'number', 'text', 'number', 'text',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>People_ID</td>\n",
       "      <td>['people id', 'name', 'height', 'weight', 'hom...</td>\n",
       "      <td>['People_ID', 'Name', 'Height', 'Weight', 'Hom...</td>\n",
       "      <td>['number', 'text', 'number', 'number', 'text']</td>\n",
       "      <td>[['perpetrator', 'People_ID', 'people', 'Peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     schema_id   table_name table_name_original     primary_key  \\\n",
       "0  perpetrator  perpetrator         perpetrator  Perpetrator_ID   \n",
       "1  perpetrator       people              people       People_ID   \n",
       "\n",
       "                                         column_list  \\\n",
       "0  ['perpetrator id', 'people id', 'date', 'year'...   \n",
       "1  ['people id', 'name', 'height', 'weight', 'hom...   \n",
       "\n",
       "                                column_list_original  \\\n",
       "0  ['Perpetrator_ID', 'People_ID', 'Date', 'Year'...   \n",
       "1  ['People_ID', 'Name', 'Height', 'Weight', 'Hom...   \n",
       "\n",
       "                                    column_datatypes  \\\n",
       "0  ['number', 'number', 'text', 'number', 'text',...   \n",
       "1     ['number', 'text', 'number', 'number', 'text']   \n",
       "\n",
       "                                        foreign_keys  \n",
       "0                                                 []  \n",
       "1  [['perpetrator', 'People_ID', 'people', 'Peopl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queryData = pd.read_csv('gs://data_tql/spider/processed/spiderQueryData.csv')\n",
    "tableData = pd.read_csv('gs://data_tql/spider/processed/Schemas/tablesSchemaSpider.csv')\n",
    "\n",
    "display(queryData.head(1))\n",
    "display(tableData.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bdd497-3c7c-42a2-809a-0f96a981a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema_natural_language(row):\n",
    "\n",
    "    schema_id = row['schema_id']\n",
    "    table_name = row['table_name']\n",
    "    primary_key = row['primary_key']\n",
    "    column_list = eval(row['column_list_original'])\n",
    "    datatype_list = eval(row['column_datatypes'])\n",
    "    foreign_key = eval(row['foreign_keys'])\n",
    "\n",
    "    column_list_with_datatype = []\n",
    "    for column, datatype in zip(column_list, datatype_list):\n",
    "        column_list_with_datatype.append(' has datatype '.join([column, datatype]))\n",
    "\n",
    "    schema_natural_language = f\"Given the Table {table_name} having columns as {', '.join(column_list_with_datatype)} which has {primary_key}\"\n",
    "    return schema_natural_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57110a2-994a-41dc-9f00-0905aa888101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many heads of the departments are older than 56 ? Given the Table department having columns as Department_ID has datatype number, Name has datatype text, Creation has datatype text, Ranking has datatype number, Budget_in_Billions has datatype number, Num_Employees has datatype number which has Department_ID and Given the Table head having columns as head_ID has datatype number, name has datatype text, born_state has datatype text, age has datatype number which has head_ID and Given the Table management having columns as department_ID has datatype number, head_ID has datatype number, temporary_acting has datatype text which has department_ID',\n",
       " 'SELECT count(*) FROM head WHERE age  >  56')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableData['schema_natural_language'] = tableData.apply(create_schema_natural_language, axis = 1)\n",
    "tableData.head(3)\n",
    "\n",
    "all_schemas = tableData['schema_id'].unique()\n",
    "schema_table_query = {}\n",
    "for schema in all_schemas:\n",
    "    schema_details = ' and '.join(tableData[tableData['schema_id'] == schema]['schema_natural_language'].values)\n",
    "    schema_table_query[schema] = schema_details\n",
    "\n",
    "queryData['schema_natural_language'] = queryData['db_id'].map(schema_table_query)\n",
    "queryData['final_TQL'] = queryData['TQL'] + ' ' + queryData['schema_natural_language']\n",
    "queryData.head(2)\n",
    "\n",
    "queryData['final_TQL'][0], queryData['SQL'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4aefe3-4f6e-4de0-b942-ddd9efff6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192a2cd6-0331-4eab-84fc-3ac081e7fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for training\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_queries, tokenizer, task_prefix):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_queries = target_queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_prefix = task_prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_text = self.task_prefix + self.input_texts[index]\n",
    "        target_query = self.target_queries[index]\n",
    "\n",
    "        input_encoding = self.tokenizer([input_text], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        target_encoding = self.tokenizer([target_query], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_encoding.input_ids.squeeze(0),\n",
    "            'attention_mask': input_encoding.attention_mask.squeeze(0),\n",
    "            'labels': target_encoding.input_ids.squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f7d609-cbde-46d0-935c-a39118708144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled dataset\n",
    "input_texts = queryData['final_TQL'].values # List of input texts\n",
    "target_queries = queryData['SQL'].values  # List of corresponding target SQL queries\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_input_texts, val_input_texts, train_target_queries, val_target_queries = train_test_split(input_texts, target_queries, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff750c0a-5be6-466f-9577-efae403074fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom dataset\n",
    "task_prefix = 'Generate an SQL Query for '\n",
    "train_dataset = SQLDataset(train_input_texts, train_target_queries, tokenizer, task_prefix)\n",
    "val_dataset = SQLDataset(val_input_texts, val_target_queries, tokenizer, task_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c277ade-2403-4a3a-be4d-a55ab8a45701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up TensorBoard writer\n",
    "writer = SummaryWriter(f'{model_folder}/logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec231dca-c416-4bc1-9f5c-ab598ca23098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112a5afd-0be9-4ebd-b8fb-217a3c2e7b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5619bb41514f44dc8d07f765b7360b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0892\n",
      "Epoch: 2, Validation Loss: 0.0475\n",
      "Epoch: 3, Validation Loss: 0.0328\n",
      "Epoch: 4, Validation Loss: 0.0252\n",
      "Epoch: 5, Validation Loss: 0.0232\n",
      "Epoch: 6, Validation Loss: 0.0206\n",
      "Epoch: 7, Validation Loss: 0.0185\n",
      "Epoch: 8, Validation Loss: 0.0187\n",
      "Epoch: 9, Validation Loss: 0.0173\n",
      "Epoch: 10, Validation Loss: 0.0166\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Write training loss to TensorBoard\n",
    "        writer.add_scalar('Training Loss', loss.item(), epoch)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    \n",
    "    # Write validation loss to TensorBoard\n",
    "    writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529d4cef-818c-4677-b4ce-6dcc762e00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{model_folder.replace(\"gs://data_tql/\",\"\")}/model.pt'\n",
    "blob_model = blob('data_tql', model_path)\n",
    "with blob_model.open(\"wb\", ignore_flush=True) as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9b1b9d-f912-42dc-81b8-f263d0633e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input text\n",
    "input_text = val_input_texts[0]\n",
    "sql = val_target_queries[0]\n",
    "\n",
    "# Tokenize and encode input text\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ed7ada-ad79-4bce-9f6d-5332c77678be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Predicted Query:  SELECT T1.first_name, T1.middle_name, count(*) FROM Students AS T1 JOIN Student_Enrolment AS T2 ON T1.student_id = T2.student_id GROUP BY T1.student_id ORDER BY count(*) DESC LIMIT 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Actual Query:  SELECT T1.student_id ,  T1.first_name ,  T1.middle_name ,  T1.last_name ,  count(*) ,  T1.student_id FROM Students AS T1 JOIN Student_Enrolment AS T2 ON T1.student_id  =  T2.student_id GROUP BY T1.student_id ORDER BY count(*) DESC LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "outputs = model.generate(input_ids=tokens.input_ids.to(device), max_new_tokens = 512)\n",
    "predicted_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(\"Input Text: \", input_text)\n",
    "print('-'*100)\n",
    "print(\"Predicted Query: \", predicted_query)\n",
    "print('-'*100)\n",
    "print(\"Actual Query: \", sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1d701-36fd-49fc-a8ad-d00157b58385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
