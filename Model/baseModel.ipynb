{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6be35e-aaf8-4750-93e5-f4076ba7a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b31101e-f99e-4805-8a04-5d17fa0b9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob(bucket_name, filename):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    return blob\n",
    "\n",
    "def get_blobs(bucket_name, folder):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = list(bucket.list_blobs(prefix=folder))\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b07dd79-33da-49f0-a4c4-05ec6070e532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://data_tql/Model/2023-10-21/t5-small/run_2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_run_date = datetime.today().strftime('%Y-%m-%d')\n",
    "model_name = 't5-small'\n",
    "model_folder = f'gs://data_tql/Model/{model_run_date}/{model_name}'\n",
    "\n",
    "folder_items = get_blobs('data_tql', model_folder.replace(\"gs://data_tql/\",\"\"))\n",
    "\n",
    "pattern = r'run_(\\d+)'\n",
    "# Initialize an empty list to store the extracted run numbers\n",
    "run_numbers = []\n",
    "# Iterate through the folder paths and extract run numbers\n",
    "for folder_path in folder_items:\n",
    "    match = re.search(pattern, folder_path.name)\n",
    "    if match:\n",
    "        run_number = int(match.group(1))\n",
    "        run_numbers.append(run_number)\n",
    "\n",
    "# Find the maximum run number\n",
    "if run_numbers:\n",
    "    run_number = max(run_numbers) + 1\n",
    "else:\n",
    "    run_number = 1\n",
    "    \n",
    "model_folder = model_folder + '/run_' + f'{run_number}'\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef33b6c9-5577-4e55-ab27-41c85185d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e0cead-4501-4614-8b39-3ec47cdfd09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>TQL</th>\n",
       "      <th>SQL</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fileName</th>\n",
       "      <th>filePath</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>How many heads of the departments are older th...</td>\n",
       "      <td>SELECT count(*) FROM head WHERE age  &gt;  56</td>\n",
       "      <td>train</td>\n",
       "      <td>department_management.sqlite</td>\n",
       "      <td>sqliteDB/department_management.sqlite</td>\n",
       "      <td>{'count(*)': {0: 5}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                                TQL  \\\n",
       "0  department_management  How many heads of the departments are older th...   \n",
       "\n",
       "                                          SQL dataset  \\\n",
       "0  SELECT count(*) FROM head WHERE age  >  56   train   \n",
       "\n",
       "                       fileName                               filePath  \\\n",
       "0  department_management.sqlite  sqliteDB/department_management.sqlite   \n",
       "\n",
       "                 result  \n",
       "0  {'count(*)': {0: 5}}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_id</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_original</th>\n",
       "      <th>primary_key</th>\n",
       "      <th>column_list</th>\n",
       "      <th>column_list_original</th>\n",
       "      <th>column_datatypes</th>\n",
       "      <th>foreign_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>Perpetrator_ID</td>\n",
       "      <td>['perpetrator id', 'people id', 'date', 'year'...</td>\n",
       "      <td>['Perpetrator_ID', 'People_ID', 'Date', 'Year'...</td>\n",
       "      <td>['number', 'number', 'text', 'number', 'text',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>People_ID</td>\n",
       "      <td>['people id', 'name', 'height', 'weight', 'hom...</td>\n",
       "      <td>['People_ID', 'Name', 'Height', 'Weight', 'Hom...</td>\n",
       "      <td>['number', 'text', 'number', 'number', 'text']</td>\n",
       "      <td>[['perpetrator', 'People_ID', 'people', 'Peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     schema_id   table_name table_name_original     primary_key  \\\n",
       "0  perpetrator  perpetrator         perpetrator  Perpetrator_ID   \n",
       "1  perpetrator       people              people       People_ID   \n",
       "\n",
       "                                         column_list  \\\n",
       "0  ['perpetrator id', 'people id', 'date', 'year'...   \n",
       "1  ['people id', 'name', 'height', 'weight', 'hom...   \n",
       "\n",
       "                                column_list_original  \\\n",
       "0  ['Perpetrator_ID', 'People_ID', 'Date', 'Year'...   \n",
       "1  ['People_ID', 'Name', 'Height', 'Weight', 'Hom...   \n",
       "\n",
       "                                    column_datatypes  \\\n",
       "0  ['number', 'number', 'text', 'number', 'text',...   \n",
       "1     ['number', 'text', 'number', 'number', 'text']   \n",
       "\n",
       "                                        foreign_keys  \n",
       "0                                                 []  \n",
       "1  [['perpetrator', 'People_ID', 'people', 'Peopl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queryData = pd.read_csv('gs://data_tql/spider/processed/spiderQueryData.csv')\n",
    "tableData = pd.read_csv('gs://data_tql/spider/processed/Schemas/tablesSchemaSpider.csv')\n",
    "\n",
    "display(queryData.head(1))\n",
    "display(tableData.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bdd497-3c7c-42a2-809a-0f96a981a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema_natural_language(row):\n",
    "\n",
    "    schema_id = row['schema_id']\n",
    "    table_name = row['table_name']\n",
    "    primary_key = row['primary_key']\n",
    "    column_list = eval(row['column_list_original'])\n",
    "    datatype_list = eval(row['column_datatypes'])\n",
    "    foreign_key = eval(row['foreign_keys'])\n",
    "\n",
    "    column_list_with_datatype = []\n",
    "    for column, datatype in zip(column_list, datatype_list):\n",
    "        column_list_with_datatype.append(' has datatype '.join([column, datatype]))\n",
    "\n",
    "    schema_natural_language = f\"Given the Table {table_name} having columns as {', '.join(column_list_with_datatype)} which has {primary_key}\"\n",
    "    return schema_natural_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57110a2-994a-41dc-9f00-0905aa888101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Given the Table department having columns as Department_ID has datatype number, Name has datatype text, Creation has datatype text, Ranking has datatype number, Budget_in_Billions has datatype number, Num_Employees has datatype number which has Department_ID and Given the Table head having columns as head_ID has datatype number, name has datatype text, born_state has datatype text, age has datatype number which has head_ID and Given the Table management having columns as department_ID has datatype number, head_ID has datatype number, temporary_acting has datatype text which has department_ID How many heads of the departments are older than 56 ?',\n",
       " 'SELECT count(*) FROM head WHERE age  >  56')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableData['schema_natural_language'] = tableData.apply(create_schema_natural_language, axis = 1)\n",
    "tableData.head(3)\n",
    "\n",
    "all_schemas = tableData['schema_id'].unique()\n",
    "schema_table_query = {}\n",
    "for schema in all_schemas:\n",
    "    schema_details = ' and '.join(tableData[tableData['schema_id'] == schema]['schema_natural_language'].values)\n",
    "    schema_table_query[schema] = schema_details\n",
    "\n",
    "queryData['schema_natural_language'] = queryData['db_id'].map(schema_table_query)\n",
    "queryData['final_TQL'] = queryData['schema_natural_language'] + ' ' + queryData['TQL']\n",
    "queryData.head(2)\n",
    "\n",
    "queryData['final_TQL'][0], queryData['SQL'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4aefe3-4f6e-4de0-b942-ddd9efff6f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f8ef53d7c74cb0af960ef0beecd6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained T5 model and tokenizer\n",
    "number_of_gpus = range(torch.cuda.device_count())\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = torch.nn.DataParallel(T5ForConditionalGeneration.from_pretrained('t5-small'), device_ids=number_of_gpus).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192a2cd6-0331-4eab-84fc-3ac081e7fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for training\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_queries, tokenizer, task_prefix):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_queries = target_queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_prefix = task_prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_text = self.task_prefix + self.input_texts[index]\n",
    "        target_query = self.target_queries[index]\n",
    "\n",
    "        input_encoding = self.tokenizer([input_text], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        target_encoding = self.tokenizer([target_query], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_encoding.input_ids.squeeze(0),\n",
    "            'attention_mask': input_encoding.attention_mask.squeeze(0),\n",
    "            'labels': target_encoding.input_ids.squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f7d609-cbde-46d0-935c-a39118708144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled dataset\n",
    "input_texts = queryData['final_TQL'].values # List of input texts\n",
    "target_queries = queryData['SQL'].values  # List of corresponding target SQL queries\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_input_texts, val_input_texts, train_target_queries, val_target_queries = train_test_split(input_texts, target_queries, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff750c0a-5be6-466f-9577-efae403074fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom dataset\n",
    "task_prefix = 'Generate an SQL Query for '\n",
    "train_dataset = SQLDataset(train_input_texts, train_target_queries, tokenizer, task_prefix)\n",
    "val_dataset = SQLDataset(val_input_texts, val_target_queries, tokenizer, task_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c277ade-2403-4a3a-be4d-a55ab8a45701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up TensorBoard writer\n",
    "writer = SummaryWriter(f'{model_folder}/logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec231dca-c416-4bc1-9f5c-ab598ca23098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb12f58-b8d1-436d-b574-38f995826ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2bb611512e4873b9d10b10702cb727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0963\n",
      "Epoch: 2, Validation Loss: 0.0525\n",
      "Epoch: 3, Validation Loss: 0.0393\n",
      "Epoch: 4, Validation Loss: 0.0319\n",
      "Epoch: 5, Validation Loss: 0.0285\n",
      "Epoch: 6, Validation Loss: 0.0262\n",
      "Epoch: 7, Validation Loss: 0.0244\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids.to(device), labels=labels.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Write training loss to TensorBoard\n",
    "        writer.add_scalar('Training Loss', loss.mean().item(), epoch)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(device), labels=labels.to(device))\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss.mean().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    \n",
    "    # Write validation loss to TensorBoard\n",
    "    writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c16536-5548-4c78-b772-18164055bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/model/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/model/generation_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/model/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [3/3 files][230.9 MiB/230.9 MiB] 100% Done                                    \n",
      "Operation completed over 3 objects/230.9 MiB.                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upload successful.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "\n",
    "local_download_path = \"/home/jupyter/model\"\n",
    "model.module.save_pretrained(local_download_path)\n",
    "\n",
    "gsutil_command = f\"gsutil -m cp -r {local_download_path} {model_folder}\"\n",
    "\n",
    "# Run the gsutil command\n",
    "try:\n",
    "    subprocess.run(gsutil_command, shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    print(\"Model upload successful.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Model upload failed. Error: {e}\")\n",
    "\n",
    "files = glob.glob(local_download_path+\"/*\")\n",
    "for f in files:\n",
    "    os.remove(f)    \n",
    "os.rmdir(local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529d4cef-818c-4677-b4ce-6dcc762e00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{model_folder.replace(\"gs://data_tql/\",\"\")}.model.pt'\n",
    "blob_model = blob('data_tql', model_path)\n",
    "with blob_model.open(\"wb\", ignore_flush=True) as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e7c4a4-31d9-47c9-b938-70d0107ba165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Model/2023-10-21/t5-small/run_4.model.pt to /home/jupyter/model/run_4.model.pt\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/config.json to /home/jupyter/model/config.json\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/generation_config.json to /home/jupyter/model/generation_config.json\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/model/config.json to /home/jupyter/model/config.json\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/model/generation_config.json to /home/jupyter/model/generation_config.json\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/model/pytorch_model.bin to /home/jupyter/model/pytorch_model.bin\n",
      "Downloaded Model/2023-10-21/t5-small/run_4/pytorch_model.bin to /home/jupyter/model/pytorch_model.bin\n",
      "All files from gs://data_tql/Model/2023-10-21/t5-small/run_4 have been downloaded to /home/jupyter/model\n"
     ]
    }
   ],
   "source": [
    "folder_items = get_blobs('data_tql', model_folder.replace(\"gs://data_tql/\",\"\"))\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "os.makedirs(local_download_path, exist_ok=True)\n",
    "\n",
    "# Download each object in the folder\n",
    "for blob in folder_items:\n",
    "    # Create the local file path by combining the local directory with the object name\n",
    "    local_file_path = os.path.join(local_download_path, blob.name.split('/')[-1])\n",
    "\n",
    "    # Download the object to the local file path\n",
    "    blob.download_to_filename(local_file_path)\n",
    "\n",
    "    print(f'Downloaded {blob.name} to {local_file_path}')\n",
    "\n",
    "print(f'All files from {model_folder} have been downloaded to {local_download_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34f4f70d-3b5b-4606-a0b9-e2b18dbfbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Predicted Query:  SELECT DISTINCT t1.authorid FROM venue AS t3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Actual Query:  SELECT DISTINCT t1.paperid FROM venue AS t2 JOIN paper AS t1 ON t2.venueid  =  t1.venueid WHERE t2.venuename  =  \"chi\";\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_download_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess input text\n",
    "input_text = val_input_texts[0]\n",
    "sql = val_target_queries[0]\n",
    "\n",
    "# Tokenize and encode input text\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=tokens.input_ids.to(device))\n",
    "predicted_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(\"Input Text: \", input_text)\n",
    "print('-'*100)\n",
    "print(\"Predicted Query: \", predicted_query)\n",
    "print('-'*100)\n",
    "print(\"Actual Query: \", sql)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
