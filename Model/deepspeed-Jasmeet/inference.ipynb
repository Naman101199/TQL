{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff2ceb2-d376-4f94-a3bb-ce42e77fb873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07174552d3c94c90acef3094f4d844a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import cast, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from transformers.integrations import deepspeed\n",
    "\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "import os\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/TQL/')\n",
    "# from utils.token import hub_token\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = (\n",
    "        \"Generate an accurate SQL query to answer the following question using only the given tables: \"\n",
    "    )\n",
    "    q_header = \"### Question\"\n",
    "    a_header = \"### Answer\"\n",
    "\n",
    "    q = example\n",
    "\n",
    "    return f\"{instruction}\\n\\n{q_header}\\n{q}\\n\\n{a_header}\\n\"\n",
    "\n",
    "def get_gpu_memory(max_gpus=None):\n",
    "    \"\"\"Get available memory for each GPU.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    gpu_memory = []\n",
    "    num_gpus = (\n",
    "        torch.cuda.device_count()\n",
    "        if max_gpus is None\n",
    "        else min(max_gpus, torch.cuda.device_count())\n",
    "    )\n",
    "\n",
    "    for gpu_id in range(num_gpus):\n",
    "        with torch.cuda.device(gpu_id):\n",
    "            device = torch.cuda.current_device()\n",
    "            gpu_properties = torch.cuda.get_device_properties(device)\n",
    "            total_memory = gpu_properties.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "            available_memory = total_memory - allocated_memory\n",
    "            gpu_memory.append(available_memory)\n",
    "    return gpu_memory\n",
    "\n",
    "def load_model(model_path, num_gpus, max_gpu_memory=None):\n",
    "    \n",
    "    kwargs = {\"torch_dtype\": torch.float16}\n",
    "    if num_gpus != 1:\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "        if max_gpu_memory is None:\n",
    "            kwargs[\n",
    "                \"device_map\"\n",
    "            ] = \"sequential\"  # This is important for not the same VRAM sizes\n",
    "            available_gpu_memory = get_gpu_memory(num_gpus)\n",
    "            kwargs[\"max_memory\"] = {\n",
    "                i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n",
    "                for i in range(num_gpus)\n",
    "            }\n",
    "        else:\n",
    "            kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n",
    "        \n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model_path = config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path, use_fast=False\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        return model, tokenizer, base_model\n",
    "\n",
    "model, tokenizer, base_model = load_model('../../../New_Model', torch.cuda.device_count(), max_gpu_memory=None)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804da4ba-63d6-483b-b5d4-e99c45a8179e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad22214-0fb9-402e-8789-8609f10922ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate an accurate SQL query to answer the following question using only the given tables: \n",
      "\n",
      "### Question\n",
      "What are the ids of the students who either registered or attended a course? CREATE TABLE addresses (address_id number, line_1 text, line_2 text, city text, zip_postcode text, state_province_county text, country text) which has address_id as primary key and CREATE TABLE people (person_id number, first_name text, middle_name text, last_name text, cell_mobile_number text, email_address text, login_name text, password text) which has person_id as primary key and CREATE TABLE students (student_id number, student_details text) which has student_id as primary key and CREATE TABLE courses (course_id text, course_name text, course_description text, other_details text) which has course_id as primary key and CREATE TABLE people addresses (person_address_id number, person_id number, address_id number, date_from time, date_to time) which has person_address_id as primary key and CREATE TABLE student course registrations (student_id number, course_id number, registration_date time) which has student_id as primary key and CREATE TABLE student course attendance (student_id number, course_id number, date_of_attendance time) which has student_id as primary key and CREATE TABLE candidates (candidate_id number, candidate_details text) which has candidate_id as primary key and CREATE TABLE candidate assessments (candidate_id number, qualification text, assessment_date time, asessment_outcome_code text) which has candidate_id as primary key\n",
      "\n",
      "### Answer\n",
      "SELECT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Actual Query:  SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 387, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "queryData = pd.read_csv('sql_queries.csv')\n",
    "question = queryData['context'][100]\n",
    "inputs = queryData['input'][100]\n",
    "sql = queryData['SQL'][100]\n",
    "\n",
    "prompt = format_example(inputs + ' ' + question)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])\n",
    "print('-'*100)\n",
    "print(\"Actual Query: \", sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8282a89-5666-4686-9b7d-2891935f178d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
