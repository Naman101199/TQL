{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff2ceb2-d376-4f94-a3bb-ce42e77fb873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 16:16:43,173] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6762cd949a2d4e9ead6686754dc081b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import cast, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from transformers.integrations import deepspeed\n",
    "\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "import os\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/TQL/')\n",
    "from utils.token import hub_token\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = (\n",
    "        \"Given the context, answer the question by writing the appropriate SQL code.\"\n",
    "    )\n",
    "    q_header = \"### Question\"\n",
    "    a_header = \"### Answer\"\n",
    "\n",
    "    q = example\n",
    "\n",
    "    return f\"{instruction}\\n\\n{q_header}\\n{q}\\n\\n{a_header}\\n\"\n",
    "\n",
    "def get_gpu_memory(max_gpus=None):\n",
    "    \"\"\"Get available memory for each GPU.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    gpu_memory = []\n",
    "    num_gpus = (\n",
    "        torch.cuda.device_count()\n",
    "        if max_gpus is None\n",
    "        else min(max_gpus, torch.cuda.device_count())\n",
    "    )\n",
    "\n",
    "    for gpu_id in range(num_gpus):\n",
    "        with torch.cuda.device(gpu_id):\n",
    "            device = torch.cuda.current_device()\n",
    "            gpu_properties = torch.cuda.get_device_properties(device)\n",
    "            total_memory = gpu_properties.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "            available_memory = total_memory - allocated_memory\n",
    "            gpu_memory.append(available_memory)\n",
    "    return gpu_memory\n",
    "\n",
    "def load_model(model_path, num_gpus, max_gpu_memory=None):\n",
    "    \n",
    "    kwargs = {\"torch_dtype\": torch.float16}\n",
    "    if num_gpus != 1:\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "        if max_gpu_memory is None:\n",
    "            kwargs[\n",
    "                \"device_map\"\n",
    "            ] = \"sequential\"  # This is important for not the same VRAM sizes\n",
    "            available_gpu_memory = get_gpu_memory(num_gpus)\n",
    "            kwargs[\"max_memory\"] = {\n",
    "                i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n",
    "                for i in range(num_gpus)\n",
    "            }\n",
    "        else:\n",
    "            kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n",
    "        \n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model_path = config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path, use_fast=False, token = hub_token\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            token = hub_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        return model, tokenizer, base_model\n",
    "\n",
    "model, tokenizer, base_model = load_model('New_Model', torch.cuda.device_count(), max_gpu_memory=None)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad22214-0fb9-402e-8789-8609f10922ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Given the context, answer the question by writing the appropriate SQL code.\n",
      "\n",
      "### Question\n",
      "Given the Table department having columns as Department_ID has datatype number, Name has datatype text, Creation has datatype text, Ranking has datatype number, Budget_in_Billions has datatype number, Num_Employees has datatype number which has Department_ID and Given the Table head having columns as head_ID has datatype number, name has datatype text, born_state has datatype text, age has datatype number which has head_ID and Given the Table management having columns as department_ID has datatype number, head_ID has datatype number, temporary_acting has datatype text which has department_ID. Answer the question by writing the appropriate SQL code. How many acting statuses are there?\n",
      "\n",
      "### Answer\n",
      " [/INST]  SELECT COUNT(*) FROM management WHERE temporary_acting = 'true';\n",
      "\n",
      "### Explanation\n",
      "The SQL code selects all rows from the management table where the temporary_acting column is true, which means it counts the number of acting statuses.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Actual Query:  SELECT count(DISTINCT temporary_acting) FROM management\n"
     ]
    }
   ],
   "source": [
    "queryData = pd.read_csv('gs://data_tql/spider/processed/spiderTrainingData.csv')\n",
    "question = queryData['question'][10]\n",
    "sql = queryData['SQL'][10]\n",
    "\n",
    "prompt = format_example(question)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\n",
    "print('-'*100)\n",
    "print(\"Actual Query: \", sql)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
